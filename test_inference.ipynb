{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1349a2ed-4eb4-4097-8b5d-f825b1899814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers.utils import export_to_video\n",
    "from causvid.data import TextDataset\n",
    "from omegaconf import OmegaConf\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62875dec-f69d-4612-9bfb-27d706278967",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"first_test\"\n",
    "logs_dir = os.path.join(\"logs\", exp_name)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "config_path = \"configs/wan_causal_dmd.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "checkpoint_folder = os.path.join(\"checkpoints\", \"autoregressive_checkpoint\")\n",
    "output_folder = logs_dir \n",
    "prompt_file_path = os.path.join(\"prompt_files\", \"prompt1.txt\")\n",
    "prompt_dataset = TextDataset(prompt_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a146f0ea-b03b-4371-9156-94d198c41119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare generator\n",
    "from causvid.models.wan.wan_wrapper import CausalWanDiffusionWrapper\n",
    "generator = CausalWanDiffusionWrapper() # this loads the base \"wan_models/Wan2.1-T2V-1.3B/\" model\n",
    "causvid_state_dict = torch.load(os.path.join(checkpoint_folder, \"model.pt\"), map_location=device)['generator'] # this loads causvid model\n",
    "generator.load_state_dict(causvid_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f6bc478-9321-418c-98e7-bc2f7298053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare text encoder\n",
    "from causvid.models.wan.wan_wrapper import WanTextEncoder\n",
    "text_encoder = WanTextEncoder() # default text encoder, in CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f5a63db-f0fe-4077-85c3-a20f20bb4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare vae\n",
    "from causvid.models.wan.wan_wrapper import WanVAEWrapper\n",
    "vae = WanVAEWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a38f1cf3-fa1c-47ea-b066-7d458fce54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VACE\n",
    "import VACE_essentials.vace_wan_model\n",
    "importlib.reload(VACE_essentials.vace_wan_model)\n",
    "from VACE_essentials.vace_wan_model import VaceWanModel\n",
    "vace_ckpt = os.path.join(\"VACE_essentials\", \"vace_wan_models\", \"Wan2.1-VACE-1.3B\")\n",
    "vace = VaceWanModel.from_pretrained(vace_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21ac333d-5da2-4a89-b604-c4222c6097c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV inference with 3 frames per block\n"
     ]
    }
   ],
   "source": [
    "import causvid.models.wan.my_inference\n",
    "importlib.reload(causvid.models.wan.my_inference)\n",
    "from causvid.models.wan.my_inference import MyInferencePipeline\n",
    "\n",
    "my_pipeline = MyInferencePipeline(generator=generator,\n",
    "                                  text_encoder=text_encoder,\n",
    "                                  vae=vae,\n",
    "                                  args=config, dtype=torch.bfloat16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a943a891-3589-4f03-819f-c8eab095a03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c1f4f4887c444d9f2b694b0650028f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampled_noise = torch.randn(\n",
    "    [1, 21, 16, 60, 104], device=device, dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "for prompt_index in tqdm(range(len(prompt_dataset))):\n",
    "    prompts = [prompt_dataset[prompt_index]]\n",
    "\n",
    "    video = my_pipeline.inference(\n",
    "        noise=sampled_noise,\n",
    "        text_prompts=prompts\n",
    "    )[0].permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "    export_to_video(\n",
    "        video, os.path.join(output_folder, f\"my_output_{prompt_index:03d}.mp4\"), fps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53d5ac30-401d-4b71-8fff-1ba95bee6c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare generator\n",
    "import causvid.models.wan.wan_wrapper\n",
    "importlib.reload(causvid.models.wan.wan_wrapper)\n",
    "importlib.reload(causvid.models.wan.vace_causal_model)\n",
    "from causvid.models.wan.wan_wrapper import VaceCausalWanDiffusionWrapper\n",
    "\n",
    "generator = VaceCausalWanDiffusionWrapper() # this loads the base \"wan_models/Wan2.1-T2V-1.3B/\" model\n",
    "generator.load_state_dict(causvid_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "339a3e01-affb-4015-a5c0-e1737b75bfa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('test_func' in dir(generator.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dec76e-5f9a-4d70-a2be-395b3610bd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "131d781c-cd50-4911-9915-3fcae546a66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV inference with 3 frames per block\n"
     ]
    }
   ],
   "source": [
    "import causvid.models.wan.my_inference\n",
    "importlib.reload(causvid.models.wan.my_inference)\n",
    "from causvid.models.wan.my_inference import MyInferencePipeline\n",
    "\n",
    "my_pipeline = MyInferencePipeline(generator=generator,\n",
    "                                  text_encoder=text_encoder,\n",
    "                                  vae=vae,\n",
    "                                  args=config, dtype=torch.bfloat16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4ca27973-61d2-4a83-8a28-8f061f9cf215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc1db8a622743a484580d20bbdf8c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt_index \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(prompt_dataset))):\n\u001b[1;32m      6\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m [prompt_dataset[prompt_index]]\n\u001b[0;32m----> 8\u001b[0m     video \u001b[38;5;241m=\u001b[39m \u001b[43mmy_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampled_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_prompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     13\u001b[0m     export_to_video(\n\u001b[1;32m     14\u001b[0m         video, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_output_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_index\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m), fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[0;32m~/CausVid/causvid/models/wan/my_inference.py:149\u001b[0m, in \u001b[0;36mMyInferencePipeline.inference\u001b[0;34m(self, noise, text_prompts, start_latents, return_latents)\u001b[0m\n\u001b[1;32m    145\u001b[0m timestep \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[1;32m    146\u001b[0m     [batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_frame_per_block], device\u001b[38;5;241m=\u001b[39mnoise\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;241m*\u001b[39m current_timestep\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenoising_step_list) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 149\u001b[0m     denoised_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoisy_image_or_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconditional_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconditional_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcrossattn_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrossattn_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_frame_per_block\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mblock_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_frame_per_block\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_seq_length\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     next_timestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenoising_step_list[index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    160\u001b[0m     noisy_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39madd_noise(\n\u001b[1;32m    161\u001b[0m         denoised_pred\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    162\u001b[0m         torch\u001b[38;5;241m.\u001b[39mrandn_like(denoised_pred\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m                    dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    166\u001b[0m     )\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m0\u001b[39m, denoised_pred\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/causvid/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/causvid/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/CausVid/causvid/models/wan/wan_wrapper.py:182\u001b[0m, in \u001b[0;36mWanDiffusionWrapper.forward\u001b[0;34m(self, noisy_image_or_video, conditional_dict, timestep, kv_cache, crossattn_cache, current_start, current_end)\u001b[0m\n\u001b[1;32m    179\u001b[0m     input_timestep \u001b[38;5;241m=\u001b[39m timestep\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     flow_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoisy_image_or_video\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_timestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcrossattn_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrossattn_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_end\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     flow_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    193\u001b[0m         noisy_image_or_video\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m    194\u001b[0m         t\u001b[38;5;241m=\u001b[39minput_timestep, context\u001b[38;5;241m=\u001b[39mprompt_embeds,\n\u001b[1;32m    195\u001b[0m         seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len\n\u001b[1;32m    196\u001b[0m     )\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/causvid/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/causvid/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/CausVid/causvid/models/wan/causal_model.py:698\u001b[0m, in \u001b[0;36mCausalWanModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    696\u001b[0m ):\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkv_cache\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_train(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/CausVid/causvid/models/wan/vace_causal_model.py:169\u001b[0m, in \u001b[0;36mVaceCausalWanModel._forward_inference\u001b[0;34m(self, x, t, context, seq_len, clip_fea, y, kv_cache, crossattn_cache, current_start, current_end)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# embeddings\u001b[39;00m\n\u001b[1;32m    168\u001b[0m x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embedding(u\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m--> 169\u001b[0m grid_sizes \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    170\u001b[0m     [torch\u001b[38;5;241m.\u001b[39mtensor(u\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m    171\u001b[0m x \u001b[38;5;241m=\u001b[39m [u\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[1;32m    172\u001b[0m seq_lens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([u\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m x], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "sampled_noise = torch.randn(\n",
    "    [1, 21, 16, 60, 104], device=device, dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "for prompt_index in tqdm(range(len(prompt_dataset))):\n",
    "    prompts = [prompt_dataset[prompt_index]]\n",
    "\n",
    "    video = my_pipeline.inference(\n",
    "        noise=sampled_noise,\n",
    "        text_prompts=prompts\n",
    "    )[0].permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "    export_to_video(\n",
    "        video, os.path.join(output_folder, f\"my_output_{prompt_index:03d}.mp4\"), fps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66c934-b001-43da-95da-3822059aca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
